{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f7d5bd3",
   "metadata": {},
   "source": [
    "# CNN on MNIST dataset\n",
    "*Graphing results for a CNN on the MNIST dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97828ddf",
   "metadata": {},
   "source": [
    "# NOTE:\n",
    "### Training the network is *significantly* faster while using high performance training on GPU with MacOS MPS (Metal Performance Shaders)\n",
    "* Training with M2 Max CPU: 88 seconds\n",
    "* Training with Google Colab T4 GPU: 39 seconds\n",
    "* Training with M2 Max MPS enabled: 25 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6521ceb",
   "metadata": {},
   "source": [
    "### Packages and Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "22319701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision #this package includes many of the most popular datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "84fd4cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "batch_size = 128\n",
    "batch_size = 32\n",
    "data_path='./data'\n",
    "\n",
    "dtype = torch.float\n",
    "# for nvidia gpus with cuda cores\n",
    "#device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# for apple m1 or m2 chips\n",
    "device = torch.device(\"mps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ab0dbea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST(root=data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = torchvision.datasets.MNIST(root=data_path, train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91b12a5",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a89a2b7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb5e145bf40>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualizing data using dataloader\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "plt.imshow(images[0].reshape(28, 28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "68b0448f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "tensor(8)\n"
     ]
    }
   ],
   "source": [
    "print(images.shape)\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d1dbbeb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(28, 28), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "               Grayscale(num_output_channels=1)\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0,), std=(1,))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print(mnist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688d2dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d131b2b",
   "metadata": {},
   "source": [
    "### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "af1d38ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=36864, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing package\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # here are the layers\n",
    "        \n",
    "        # these are convolutional 2d layers. the numbers represent input, output, and kernel size, respectively\n",
    "        # the kernel size is the size of the filter that is slided over the image, in this case, 3x3\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        self.fc1 = nn.Linear(64 * 24 * 24, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "346b1eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e633aa",
   "metadata": {},
   "source": [
    "### Net Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2a74e65c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    50] loss: 0.057 | time: 0.85s\n",
      "[1,   100] loss: 0.050 | time: 1.36s\n",
      "[1,   150] loss: 0.028 | time: 1.87s\n",
      "[1,   200] loss: 0.014 | time: 2.37s\n",
      "[1,   250] loss: 0.012 | time: 2.88s\n",
      "[1,   300] loss: 0.010 | time: 3.38s\n",
      "[1,   350] loss: 0.010 | time: 3.87s\n",
      "[1,   400] loss: 0.010 | time: 4.38s\n",
      "[1,   450] loss: 0.009 | time: 4.88s\n",
      "[1,   500] loss: 0.010 | time: 5.38s\n",
      "[1,   550] loss: 0.010 | time: 5.89s\n",
      "[1,   600] loss: 0.007 | time: 6.38s\n",
      "[1,   650] loss: 0.009 | time: 6.88s\n",
      "[1,   700] loss: 0.008 | time: 7.39s\n",
      "[1,   750] loss: 0.009 | time: 7.89s\n",
      "[1,   800] loss: 0.009 | time: 8.39s\n",
      "[1,   850] loss: 0.007 | time: 8.89s\n",
      "[1,   900] loss: 0.007 | time: 9.39s\n",
      "[1,   950] loss: 0.008 | time: 9.89s\n",
      "[1,  1000] loss: 0.008 | time: 10.39s\n",
      "[1,  1050] loss: 0.007 | time: 10.90s\n",
      "[1,  1100] loss: 0.006 | time: 11.40s\n",
      "[1,  1150] loss: 0.006 | time: 11.90s\n",
      "[1,  1200] loss: 0.007 | time: 12.40s\n",
      "[1,  1250] loss: 0.007 | time: 12.90s\n",
      "[1,  1300] loss: 0.006 | time: 13.40s\n",
      "[1,  1350] loss: 0.006 | time: 13.90s\n",
      "[1,  1400] loss: 0.006 | time: 14.40s\n",
      "[1,  1450] loss: 0.005 | time: 14.90s\n",
      "[1,  1500] loss: 0.006 | time: 15.40s\n",
      "[1,  1550] loss: 0.005 | time: 15.90s\n",
      "[1,  1600] loss: 0.005 | time: 16.40s\n",
      "[1,  1650] loss: 0.004 | time: 16.90s\n",
      "[1,  1700] loss: 0.005 | time: 17.43s\n",
      "[1,  1750] loss: 0.005 | time: 17.93s\n",
      "[1,  1800] loss: 0.005 | time: 18.43s\n",
      "[1,  1850] loss: 0.005 | time: 18.93s\n",
      "[2,    50] loss: 0.005 | time: 19.68s\n",
      "[2,   100] loss: 0.005 | time: 20.18s\n",
      "[2,   150] loss: 0.004 | time: 20.68s\n",
      "[2,   200] loss: 0.005 | time: 21.19s\n",
      "[2,   250] loss: 0.004 | time: 21.69s\n",
      "[2,   300] loss: 0.004 | time: 22.19s\n",
      "[2,   350] loss: 0.005 | time: 22.69s\n",
      "[2,   400] loss: 0.004 | time: 23.19s\n",
      "[2,   450] loss: 0.004 | time: 23.69s\n",
      "[2,   500] loss: 0.004 | time: 24.19s\n",
      "[2,   550] loss: 0.004 | time: 24.69s\n",
      "[2,   600] loss: 0.004 | time: 25.19s\n",
      "[2,   650] loss: 0.004 | time: 25.70s\n",
      "[2,   700] loss: 0.003 | time: 26.21s\n",
      "[2,   750] loss: 0.004 | time: 26.72s\n",
      "[2,   800] loss: 0.004 | time: 27.22s\n",
      "[2,   850] loss: 0.003 | time: 27.73s\n",
      "[2,   900] loss: 0.003 | time: 28.23s\n",
      "[2,   950] loss: 0.003 | time: 28.73s\n",
      "[2,  1000] loss: 0.004 | time: 29.22s\n",
      "[2,  1050] loss: 0.004 | time: 29.72s\n",
      "[2,  1100] loss: 0.004 | time: 30.22s\n",
      "[2,  1150] loss: 0.004 | time: 30.72s\n",
      "[2,  1200] loss: 0.005 | time: 31.22s\n",
      "[2,  1250] loss: 0.004 | time: 31.73s\n",
      "[2,  1300] loss: 0.003 | time: 32.23s\n",
      "[2,  1350] loss: 0.005 | time: 32.73s\n",
      "[2,  1400] loss: 0.003 | time: 33.23s\n",
      "[2,  1450] loss: 0.003 | time: 33.74s\n",
      "[2,  1500] loss: 0.003 | time: 34.24s\n",
      "[2,  1550] loss: 0.004 | time: 34.74s\n",
      "[2,  1600] loss: 0.004 | time: 35.24s\n",
      "[2,  1650] loss: 0.004 | time: 35.74s\n",
      "[2,  1700] loss: 0.004 | time: 36.24s\n",
      "[2,  1750] loss: 0.003 | time: 36.74s\n",
      "[2,  1800] loss: 0.004 | time: 37.24s\n",
      "[2,  1850] loss: 0.003 | time: 37.74s\n",
      "Finished Training\n",
      "Training took 38.00 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        \n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 50 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f} | time: {time.time() - start_time:.2f}s')\n",
    "            running_loss = 0.0\n",
    "end_time = time.time()  # Record end time\n",
    "print('Finished Training')\n",
    "print(f\"Training took {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b648dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "PATH = './model/mnist_cnn_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6148d151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6be6bb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=36864, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a547d1f",
   "metadata": {},
   "source": [
    "### Accuracy Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b683cb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 96.68%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # We don't need gradients for evaluation\n",
    "    for data in test_loader:  # Assuming test_loader is your validation/test data loader\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # Get the predicted class for each sample in the batch\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Update total number of samples and number of correct predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f0efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scnncifar10",
   "language": "python",
   "name": "scnncifar10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
